{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6670dddd",
   "metadata": {},
   "source": [
    "## EEG and KAN for HMS Brain Classificatio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3abb06-7e35-4718-8574-ae3900e06436",
   "metadata": {},
   "source": [
    "### CELL 1: COMPLETE SETUP (GPU, SEED, DATA, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7641e1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] Loading libraries... âœ“\n",
      "\n",
      "[DEVICE] Checking GPU...\n",
      "  CUDA available: True\n",
      "  GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "  Using: GPU âœ“\n",
      "\n",
      "[SEED] Setting random seeds...\n",
      "  Random seed: 42 âœ“\n",
      "\n",
      "[PATHS] Checking data directories...\n",
      "  data_package âœ“\n",
      "  spec_hr_out âœ“\n",
      "\n",
      "============================================================\n",
      "SETUP COMPLETE âœ“\n",
      "============================================================\n",
      "\n",
      "[DATA] Loading metadata and labels...\n",
      "\n",
      "  Meta: (17089, 3)\n",
      "  Labels: (17089, 6)\n",
      "  Classes: ['seizure', 'lpd', 'gpd', 'lrda', 'grda', 'other']\n",
      "\n",
      "  Class distribution:\n",
      "    seizure   :  2716 ( 15.9%)\n",
      "    lpd       :  2583 ( 15.1%)\n",
      "    gpd       :  1814 ( 10.6%)\n",
      "    lrda      :   936 (  5.5%)\n",
      "    grda      :  1835 ( 10.7%)\n",
      "    other     :  7205 ( 42.2%)\n",
      "\n",
      "[FOLDS] Creating stratified folds...\n",
      "  5 folds created\n",
      "  Fold 0: train=13671, val=3418\n",
      "\n",
      "============================================================\n",
      "DATA LOADED âœ“\n",
      "============================================================\n",
      "\n",
      "[DATASET] Creating dataset class...\n",
      "\n",
      "  Test dataset: 50 samples\n",
      "  Test batch: x=torch.Size([4, 4, 224, 224]), y=torch.Size([4, 6]), w=torch.Size([4])\n",
      "\n",
      "============================================================\n",
      "DATASET READY âœ“\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "========== âœ… ALL SETUP COMPLETE - READY TO TRAIN! ==========\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"[SETUP] Loading libraries... âœ“\\n\")\n",
    "\n",
    "# ===== GPU SETUP =====\n",
    "print(\"[DEVICE] Checking GPU...\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"  Using: GPU âœ“\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"  Using: CPU\")\n",
    "\n",
    "# ===== REPRODUCIBILITY =====\n",
    "print(\"\\n[SEED] Setting random seeds...\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"  Random seed: 42 âœ“\")\n",
    "\n",
    "# ===== PATHS =====\n",
    "print(\"\\n[PATHS] Checking data directories...\")\n",
    "DATA_PKG = Path(\"data_package\")\n",
    "SPEC_DIR = Path(\"spec_hr_out\")\n",
    "\n",
    "assert DATA_PKG.exists(), \"âŒ data_package/ not found\"\n",
    "assert SPEC_DIR.exists(), \"âŒ spec_hr_out/ not found\"\n",
    "print(f\"  {DATA_PKG} âœ“\")\n",
    "print(f\"  {SPEC_DIR} âœ“\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SETUP COMPLETE âœ“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== LOAD DATA =====\n",
    "print(\"\\n[DATA] Loading metadata and labels...\\n\")\n",
    "\n",
    "meta_use = pd.read_csv(DATA_PKG / \"meta_use.csv\")\n",
    "print(f\"  Meta: {meta_use.shape}\")\n",
    "\n",
    "lbl = np.load(DATA_PKG / \"labels.npz\", allow_pickle=True)\n",
    "y_soft  = lbl[\"y_soft\"]\n",
    "w_conf  = lbl[\"w_conf\"]\n",
    "classes = [c for c in lbl[\"classes\"]]\n",
    "\n",
    "print(f\"  Labels: {y_soft.shape}\")\n",
    "print(f\"  Classes: {classes}\")\n",
    "\n",
    "# Class distribution\n",
    "y_hard = y_soft.argmax(axis=1)\n",
    "print(\"\\n  Class distribution:\")\n",
    "for i, cls in enumerate(classes):\n",
    "    count = (y_hard == i).sum()\n",
    "    print(f\"    {cls:10s}: {count:5d} ({100*count/len(y_hard):5.1f}%)\")\n",
    "\n",
    "# ===== CREATE FOLDS =====\n",
    "print(\"\\n[FOLDS] Creating stratified folds...\")\n",
    "N_SPLIT = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLIT, shuffle=True, random_state=42)\n",
    "folds = list(skf.split(meta_use, y_hard))\n",
    "print(f\"  {len(folds)} folds created\")\n",
    "print(f\"  Fold 0: train={len(folds[0][0])}, val={len(folds[0][1])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA LOADED âœ“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== DATASET CLASS =====\n",
    "print(\"\\n[DATASET] Creating dataset class...\\n\")\n",
    "\n",
    "class SpecDataset(Dataset):\n",
    "    \"\"\"EEG Spectrogram Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, df, root_dir, y_soft, w_conf, F_target=81, T_target=600):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root = Path(root_dir)\n",
    "        self.y_soft = y_soft\n",
    "        self.w_conf = w_conf\n",
    "        self.F_target = F_target\n",
    "        self.T_target = T_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _center_crop_pad(self, x):\n",
    "        C, F, T = x.shape\n",
    "        # Frequency\n",
    "        if F >= self.F_target:\n",
    "            f0 = (F - self.F_target) // 2\n",
    "            x = x[:, f0:f0+self.F_target, :]\n",
    "        else:\n",
    "            pad = self.F_target - F\n",
    "            x = np.pad(x, ((0,0),(pad//2, pad-pad//2),(0,0)), mode=\"constant\")\n",
    "        # Time\n",
    "        if T >= self.T_target:\n",
    "            t0 = (T - self.T_target) // 2\n",
    "            x = x[:, :, t0:t0+self.T_target]\n",
    "        else:\n",
    "            pad = self.T_target - T\n",
    "            x = np.pad(x, ((0,0),(0,0),(pad//2, pad-pad//2)), mode=\"constant\")\n",
    "        return x.copy()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        eid = int(row.eeg_id)\n",
    "        npz = np.load(self.root / f\"{eid}_hr.npz\")\n",
    "        x = npz[\"x\"]\n",
    "        x = self._center_crop_pad(x)\n",
    "        x = torch.from_numpy(x).float()\n",
    "        # Resize to 224x224\n",
    "        x = F.interpolate(x.unsqueeze(0), size=(224,224),\n",
    "                          mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        y = torch.from_numpy(self.y_soft[self.df.index[idx]]).float()\n",
    "        w = torch.tensor(self.w_conf[self.df.index[idx]], dtype=torch.float32)\n",
    "        return x, y, w\n",
    "\n",
    "# Test dataset\n",
    "tr_idx, va_idx = folds[0]\n",
    "ds_test = SpecDataset(meta_use.iloc[tr_idx[:50]], SPEC_DIR, y_soft, w_conf)\n",
    "print(f\"  Test dataset: {len(ds_test)} samples\")\n",
    "\n",
    "dl_test = DataLoader(ds_test, batch_size=4, shuffle=True, num_workers=0, pin_memory=False)\n",
    "xb, yb, wb = next(iter(dl_test))\n",
    "print(f\"  Test batch: x={xb.shape}, y={yb.shape}, w={wb.shape}\")\n",
    "\n",
    "del dl_test, ds_test, xb, yb, wb\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET READY âœ“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" âœ… ALL SETUP COMPLETE - READY TO TRAIN! \".center(60, \"=\"))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15993fe",
   "metadata": {},
   "source": [
    "### CELL 2:EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f3add9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EEGNet FIXED!\n"
     ]
    }
   ],
   "source": [
    "class EEGNet_4Ch(nn.Module):\n",
    "    \"\"\"EEGNet for 4-channel EEG spectrograms - FIXED\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=6, n_channels=4, F1=8, F2=16, D=2, dropout=0.25, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: Temporal convolution\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, F1, (1, 64), padding=(0, 32), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "        )\n",
    "        \n",
    "        # Block 2: Depthwise spatial convolution\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(F1, F1 * D, (n_channels, 1), groups=F1, bias=False),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Block 3: Separable convolution\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F1 * D, (1, 16), padding=(0, 8), groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F1 * D, F2, (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # âœ… FIX: Adaptive pooling ensures consistent size!\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Classifier - now just F2 features\n",
    "        self.classifier = nn.Linear(F2, n_classes)  # 16 -> 6\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)           # [B, 8, 224, 224]\n",
    "        x = self.block2(x)           # [B, 16, 1, 56]\n",
    "        x = self.block3(x)           # [B, 16, 1, ?]\n",
    "        x = self.adaptive_pool(x)    # [B, 16, 1, 1] âœ… FIXED!\n",
    "        x = x.view(x.size(0), -1)    # [B, 16]\n",
    "        return self.classifier(x)    # [B, 6]\n",
    "\n",
    "print(\"âœ… EEGNet FIXED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0acf9-0431-45ee-b934-e4c533794521",
   "metadata": {},
   "source": [
    "### CELL 3: KAN ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119850d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KAN defined\n"
     ]
    }
   ],
   "source": [
    "class KANLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, grid_size=5, spline_order=3):\n",
    "        super().__init__()\n",
    "        self.coefficients = nn.Parameter(torch.randn(out_features, in_features, grid_size) * 0.1)\n",
    "        self.base_weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        self.register_buffer('grid', torch.linspace(-1, 1, grid_size))\n",
    "        self.spline_order = spline_order\n",
    "    \n",
    "    def b_spline_basis(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        grid = self.grid.view(1, 1, -1)\n",
    "        distances = torch.abs(x - grid)\n",
    "        basis = torch.relu(1 - distances) ** self.spline_order\n",
    "        return basis / (basis.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        basis = self.b_spline_basis(x)\n",
    "        return torch.einsum('bik,oik->bo', basis, self.coefficients) + F.linear(x, self.base_weight)\n",
    "\n",
    "class KAN_4Ch(nn.Module):\n",
    "    def __init__(self, n_classes=6, n_channels=4, grid_size=5, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        self.kan1 = KANLayer(256, 128, grid_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.kan2 = KANLayer(128, 64, grid_size)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.output = nn.Linear(64, n_classes)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight) if isinstance(m, nn.Conv2d) else nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x).view(x.size(0), -1)\n",
    "        x = self.dropout1(self.kan1(x))\n",
    "        x = self.dropout2(self.kan2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "print(\"âœ… KAN defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449f4df3",
   "metadata": {},
   "source": [
    "### CELL4:Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296e90fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training components ready\n"
     ]
    }
   ],
   "source": [
    "class SoftFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=3.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma = alpha, gamma\n",
    "    \n",
    "    def forward(self, logits, soft_targets, sample_weights=None):\n",
    "        hard_targets = soft_targets.argmax(dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        p_t = probs.gather(1, hard_targets.unsqueeze(1)).squeeze(1)\n",
    "        loss = ((1 - p_t) ** self.gamma) * (-(soft_targets * F.log_softmax(logits, dim=1)).sum(dim=1))\n",
    "        if self.alpha is not None:\n",
    "            loss = self.alpha[hard_targets] * loss\n",
    "        if sample_weights is not None:\n",
    "            loss = loss * sample_weights\n",
    "        return loss.mean()\n",
    "\n",
    "def create_hybrid_loader(fold=0, target_ratio=0.4, weight_power=3.0, batch_size=16):\n",
    "    tr_idx, va_idx = folds[fold]\n",
    "    df_tr = meta_use.iloc[tr_idx]\n",
    "    y_soft_tr, w_conf_tr = y_soft[tr_idx], w_conf[tr_idx]\n",
    "    \n",
    "    y_hard = y_soft_tr.argmax(axis=1)\n",
    "    counts = np.bincount(y_hard, minlength=6)\n",
    "    target = int(counts.max() * target_ratio)\n",
    "    \n",
    "    indices_add = []\n",
    "    for i in range(6):\n",
    "        mask = y_hard == i\n",
    "        if mask.sum() < target:\n",
    "            idx = np.where(mask)[0]\n",
    "            indices_add.extend(np.random.choice(idx, target - mask.sum(), True))\n",
    "    \n",
    "    all_idx = np.concatenate([np.arange(len(y_hard)), indices_add])\n",
    "    np.random.shuffle(all_idx)\n",
    "    \n",
    "    df_tr_over = df_tr.iloc[all_idx].reset_index(drop=True)\n",
    "    y_soft_over, w_conf_over = y_soft_tr[all_idx], w_conf_tr[all_idx]\n",
    "    \n",
    "    y_hard_over = y_soft_over.argmax(axis=1)\n",
    "    counts_over = np.bincount(y_hard_over, minlength=6)\n",
    "    weights = (len(y_hard_over) / (counts_over + 1)) ** weight_power\n",
    "    weights = torch.FloatTensor(weights / weights.sum() * 6)\n",
    "    \n",
    "    sampler = WeightedRandomSampler(weights[y_hard_over].numpy(), len(y_hard_over), True)\n",
    "    \n",
    "    ds_tr = SpecDataset(df_tr_over, SPEC_DIR, y_soft_over, w_conf_over)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size, sampler=sampler, num_workers=0)\n",
    "    \n",
    "    ds_va = SpecDataset(meta_use.iloc[va_idx], SPEC_DIR, y_soft[va_idx], w_conf[va_idx])\n",
    "    dl_va = DataLoader(ds_va, batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return dl_tr, dl_va, weights\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_full(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    for x, y, w in loader:\n",
    "        preds.append(model(x.to(device)).argmax(1).cpu().numpy())\n",
    "        targets.append(y.argmax(1).cpu().numpy())\n",
    "    y_pred, y_true = np.concatenate(preds), np.concatenate(targets)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "print(\"âœ… Training components ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da46f0-2018-4733-be89-b8596d36082f",
   "metadata": {},
   "source": [
    "### CELL 5:# CELL 5: TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dabfded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training function ready\n"
     ]
    }
   ],
   "source": [
    "def train_model(model_name, model_class, fold=0, epochs=30, batch_size=16, base_lr=3e-4, gamma=3.0, patience=5):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\" TRAINING {model_name} \".center(80, \"=\"))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n[1/5] Creating dataloaders...\")\n",
    "    dl_tr, dl_va, class_weights = create_hybrid_loader(fold, batch_size=batch_size)\n",
    "    print(f\"âœ“ Train: {len(dl_tr.dataset)}, Valid: {len(dl_va.dataset)}\")\n",
    "    \n",
    "    print(\"\\n[2/5] Creating model...\")\n",
    "    model = model_class(n_classes=6, pretrained=False).to(device)\n",
    "    print(f\"âœ“ Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "    \n",
    "    print(f\"\\n[3/5] Setup (Focal gamma={gamma})...\")\n",
    "    criterion = SoftFocalLoss(class_weights.to(device), gamma)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), base_lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    \n",
    "    print(\"\\n[4/5] Training...\")\n",
    "    print(f\"{'Ep':<4s} {'Loss':>8s} {'Acc':>8s} {'Prec':>8s} {'Rec':>8s} {'F1':>8s} {'Time':>6s} {'Status':<10s}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    best_f1, best_state, no_improve = 0.0, None, 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss, n = 0.0, 0\n",
    "        for x, y, w in dl_tr:\n",
    "            x, y, w = x.to(device), y.to(device), w.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y, w)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            n += x.size(0)\n",
    "        \n",
    "        train_loss /= n\n",
    "        val_results = evaluate_full(model, dl_va)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_results['f1_macro'] > best_f1:\n",
    "            best_f1 = val_results['f1_macro']\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "            status = \"âœ“ BEST\"\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            status = f\"({no_improve}/{patience})\"\n",
    "        \n",
    "        print(f\"{epoch:02d}  {train_loss:8.4f} {val_results['accuracy']:8.4f} {val_results['precision_macro']:8.4f} \"\n",
    "              f\"{val_results['recall_macro']:8.4f} {val_results['f1_macro']:8.4f} {time.time()-t0:5.0f}s  {status}\")\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            print(f\"\\nEarly stop at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    print(\"\\n[5/5] Final evaluation...\")\n",
    "    final = evaluate_full(model, dl_va)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\" {model_name} RESULTS \".center(80, \"=\"))\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Accuracy:  {final['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {final['precision_macro']:.4f}\")\n",
    "    print(f\"Recall:    {final['recall_macro']:.4f}\")\n",
    "    print(f\"F1 Score:  {final['f1_macro']:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    safe_name = model_name.lower().replace(\" \", \"_\").replace(\"-\", \"\")\n",
    "    torch.save(model.state_dict(), f'{safe_name}_best.pth')\n",
    "    print(f\"\\nðŸ’¾ Saved: {safe_name}_best.pth\")\n",
    "    \n",
    "    return model, final\n",
    "\n",
    "print(\"âœ… Training function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b0fec",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddbe6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "=============================== TRAINING EEGNet ================================\n",
      "================================================================================\n",
      "\n",
      "[1/5] Creating dataloaders...\n",
      "âœ“ Train: 17289, Valid: 3418\n",
      "\n",
      "[2/5] Creating model...\n",
      "âœ“ Parameters: 0.00M\n",
      "\n",
      "[3/5] Setup (Focal gamma=3.0)...\n",
      "\n",
      "[4/5] Training...\n",
      "Ep       Loss      Acc     Prec      Rec       F1   Time Status    \n",
      "--------------------------------------------------------------------------------\n",
      "01    1.1628   0.1811   0.2208   0.2662   0.1525   553s  âœ“ BEST\n",
      "02    1.0803   0.2025   0.2240   0.2928   0.1888   112s  âœ“ BEST\n",
      "03    1.0311   0.2367   0.2063   0.3452   0.2393    69s  âœ“ BEST\n",
      "04    0.9984   0.2238   0.2199   0.3448   0.2380    61s  (1/5)\n",
      "05    0.9900   0.2589   0.2167   0.3667   0.2628    58s  âœ“ BEST\n",
      "06    0.9734   0.2575   0.2223   0.3741   0.2702    54s  âœ“ BEST\n",
      "07    0.9387   0.2402   0.2302   0.3708   0.2632    57s  (1/5)\n",
      "08    0.9337   0.2618   0.2435   0.3853   0.2843    52s  âœ“ BEST\n",
      "09    0.9259   0.2659   0.2207   0.3623   0.2720    49s  (1/5)\n",
      "10    0.9259   0.2923   0.2429   0.4110   0.2992    49s  âœ“ BEST\n",
      "11    0.8966   0.2838   0.2535   0.4126   0.2941    71s  (1/5)\n",
      "12    0.8994   0.2905   0.2495   0.4102   0.3041    48s  âœ“ BEST\n",
      "13    0.8933   0.2867   0.2697   0.4279   0.3101    47s  âœ“ BEST\n",
      "14    0.8807   0.2879   0.2586   0.4189   0.2987    47s  (1/5)\n",
      "15    0.8662   0.2970   0.2634   0.4386   0.3141    45s  âœ“ BEST\n",
      "16    0.8676   0.3005   0.2596   0.4360   0.3098    46s  (1/5)\n",
      "17    0.8657   0.2747   0.2797   0.4262   0.3071    45s  (2/5)\n",
      "18    0.8522   0.3113   0.2705   0.4547   0.3220    44s  âœ“ BEST\n",
      "19    0.8539   0.2776   0.2748   0.4261   0.3054    45s  (1/5)\n",
      "20    0.8535   0.3090   0.2695   0.4546   0.3203    45s  (2/5)\n",
      "21    0.8521   0.3092   0.2661   0.4495   0.3189    43s  (3/5)\n",
      "22    0.8416   0.3095   0.2753   0.4600   0.3241    42s  âœ“ BEST\n",
      "23    0.8344   0.3040   0.2722   0.4420   0.3135    45s  (1/5)\n",
      "24    0.8358   0.2855   0.2793   0.4310   0.3076    42s  (2/5)\n",
      "25    0.8400   0.3051   0.2758   0.4455   0.3183    42s  (3/5)\n",
      "26    0.8413   0.3154   0.2716   0.4629   0.3281    42s  âœ“ BEST\n",
      "27    0.8278   0.2902   0.2731   0.4348   0.3090    42s  (1/5)\n",
      "28    0.8363   0.3060   0.2714   0.4528   0.3214    43s  (2/5)\n",
      "29    0.8243   0.3054   0.2762   0.4482   0.3183    42s  (3/5)\n",
      "30    0.8295   0.3013   0.2756   0.4540   0.3209    42s  (4/5)\n",
      "\n",
      "[5/5] Final evaluation...\n",
      "\n",
      "================================================================================\n",
      "================================ EEGNet RESULTS ================================\n",
      "================================================================================\n",
      "Accuracy:  0.3154\n",
      "Precision: 0.2716\n",
      "Recall:    0.4629\n",
      "F1 Score:  0.3281\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¾ Saved: eegnet_best.pth\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "model_eegnet, results['EEGNet'] = train_model('EEGNet', EEGNet_4Ch, epochs=30, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be22a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "================================= TRAINING KAN =================================\n",
      "================================================================================\n",
      "\n",
      "[1/5] Creating dataloaders...\n",
      "âœ“ Train: 17289, Valid: 3418\n",
      "\n",
      "[2/5] Creating model...\n",
      "âœ“ Parameters: 0.64M\n",
      "\n",
      "[3/5] Setup (Focal gamma=3.0)...\n",
      "\n",
      "[4/5] Training...\n",
      "Ep       Loss      Acc     Prec      Rec       F1   Time Status    \n",
      "--------------------------------------------------------------------------------\n",
      "01    2.2846   0.1878   0.2778   0.3170   0.2170    98s  âœ“ BEST\n",
      "02    1.4887   0.2420   0.2912   0.3655   0.2553    61s  âœ“ BEST\n",
      "03    1.1565   0.2689   0.3387   0.4115   0.2800    58s  âœ“ BEST\n",
      "04    0.9416   0.3031   0.2744   0.4489   0.3098   133s  âœ“ BEST\n",
      "05    0.8355   0.3189   0.2893   0.4649   0.3327    72s  âœ“ BEST\n",
      "06    0.7712   0.3344   0.3331   0.5061   0.3645    62s  âœ“ BEST\n",
      "07    0.7204   0.2616   0.3292   0.4177   0.2982    59s  (1/5)\n",
      "08    0.6937   0.3607   0.3153   0.5091   0.3726    59s  âœ“ BEST\n",
      "09    0.6623   0.3069   0.3379   0.4654   0.3288    58s  (1/5)\n",
      "10    0.6432   0.3444   0.2750   0.4665   0.3306    56s  (2/5)\n",
      "11    0.6360   0.3625   0.3306   0.5389   0.3894    57s  âœ“ BEST\n",
      "12    0.6234   0.3537   0.3293   0.5128   0.3759    58s  (1/5)\n",
      "13    0.5891   0.3736   0.3553   0.5475   0.4072    56s  âœ“ BEST\n",
      "14    0.5821   0.3792   0.3409   0.5415   0.3991    56s  (1/5)\n",
      "15    0.5651   0.3411   0.3692   0.5016   0.3810    68s  (2/5)\n",
      "16    0.5378   0.3654   0.3072   0.5162   0.3692    55s  (3/5)\n",
      "17    0.5124   0.3777   0.3485   0.5574   0.4073    55s  âœ“ BEST\n",
      "18    0.5094   0.3739   0.3412   0.5355   0.3966    53s  (1/5)\n",
      "19    0.4941   0.3882   0.3204   0.5529   0.3990    55s  (2/5)\n",
      "20    0.4693   0.3827   0.3417   0.5563   0.4068    55s  (3/5)\n",
      "21    0.4496   0.3859   0.3319   0.5547   0.4005    56s  (4/5)\n",
      "22    0.4483   0.3821   0.3298   0.5525   0.4001    53s  (5/5)\n",
      "\n",
      "Early stop at epoch 22\n",
      "\n",
      "[5/5] Final evaluation...\n",
      "\n",
      "================================================================================\n",
      "================================= KAN RESULTS ==================================\n",
      "================================================================================\n",
      "Accuracy:  0.3777\n",
      "Precision: 0.3485\n",
      "Recall:    0.5574\n",
      "F1 Score:  0.4073\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¾ Saved: kan_best.pth\n"
     ]
    }
   ],
   "source": [
    "model_kan, results['KAN'] = train_model('KAN', KAN_4Ch, epochs=30, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib",
   "language": "python",
   "name": "anomalib100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
